{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto-refresh helper functions\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import KDTree\n",
    "import pynndescent\n",
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class DKNN(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, k, alpha=1, beta=1):\n",
    "        super().__init__()\n",
    "        self.k = k          # 'k' neighbors\n",
    "        self.A = None       # PSD matrix objective\n",
    "        self.pi = None      # technically log(pi)\n",
    "        self.trees = []     # search tree for NN\n",
    "        \n",
    "        # importance weights for each class (k,)\n",
    "        if type(alpha) in {float, int}:\n",
    "            self.alpha = np.full(k, alpha)\n",
    "        else:\n",
    "            assert k == len(alpha)\n",
    "            self.alpha = alpha\n",
    "        self.beta  = beta   # regularization term\n",
    "\n",
    "    # Mahalanobis distance\n",
    "    def dist(self, x, mu, c):\n",
    "        delta = x - mu\n",
    "        return np.sum(np.multiply(delta @ self.A, delta), axis=-1) - self.pi[c]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.trees = [] # reset for each fit -- when using CV need this\n",
    "        self.X = X\n",
    "        self.C = np.unique(y)\n",
    "        self.classes_ = self.C\n",
    "        self.c_idx = []  # indices belonging to 'c' w.r.t full training X\n",
    "        for ci in self.C:\n",
    "            self.c_idx.append(np.where(y == ci))\n",
    "        n, d = X.shape\n",
    "\n",
    "        centroids = []\n",
    "\n",
    "        # Find centroids of class C[i]\n",
    "        for idx in self.c_idx:\n",
    "            # Get k nearest neighbors of class C[i] for all training data X\n",
    "\n",
    "            # tree = KDTree(X[idx])\n",
    "            # _, n_idx = tree.query(X, self.k)\n",
    "            # self.trees.append(tree)\n",
    "\n",
    "            index = pynndescent.NNDescent(X[idx])\n",
    "            index.prepare()\n",
    "            self.trees.append(index)\n",
    "\n",
    "            n_idx, _ = index.query(X)\n",
    "            # Take k nearest neighbors\n",
    "            n_idx = n_idx[:, :self.k]\n",
    "\n",
    "            # Compute centroids\n",
    "            neighbors = X[idx][n_idx] # X[of class 'c'][its nearest neighbors w.r.t X[c]]\n",
    "            if self.k == 1:\n",
    "                centroid_c = neighbors\n",
    "            else:\n",
    "                centroid_c = np.mean(neighbors, axis=1)\n",
    "            centroids.append(centroid_c)\n",
    "        \n",
    "        centroids = np.stack(centroids, axis=0)\n",
    "\n",
    "        # Convex problem formulation\n",
    "        self.pi = np.array([len(idx[0]) / n for idx in self.c_idx])\n",
    "        self.A = cp.Variable((d, d))\n",
    "\n",
    "        delta = X - centroids\n",
    "\n",
    "        # should work\n",
    "        # f_mult = np.sum(np.multiply(delta @ self.A, delta), axis=2) - self.pi[:, np.newaxis]\n",
    "        # print(f_mult[0, 0])\n",
    "\n",
    "        constraints = []\n",
    "        epsilon = cp.Variable(n)\n",
    "        constraints.append(epsilon >= 0)\n",
    "\n",
    "        for i in range(n):\n",
    "            for c in self.C:\n",
    "                if c == y[i]:\n",
    "                    continue\n",
    "                constraints += [\n",
    "                    delta[y[i], i] @ self.A @ delta[y[i], i].T - cp.log(self.pi[y[i]]) + 1 - epsilon[i] \n",
    "                    <= delta[c, i] @ self.A @ delta[c   , i].T - cp.log(self.pi[c])\n",
    "                ]\n",
    "        \n",
    "        # alpha_vec = np.array([self.alpha[i] for i in range(len(y))])  # corresponding class importance weight\n",
    "        objective = cp.Minimize(cp.sum(cp.multiply(1, epsilon)) + self.beta * cp.norm(self.A, 1))\n",
    "\n",
    "        prob = cp.Problem(objective, constraints)\n",
    "        prob.solve()\n",
    "\n",
    "        self.A = self.A.value\n",
    "\n",
    "    def predict(self, X_new):\n",
    "        if X_new.ndim == 1:\n",
    "            n = 1\n",
    "        else:\n",
    "            n = X_new.shape[0]\n",
    "\n",
    "        dist_c = np.empty((n, len(self.trees)))\n",
    "        for c, t in enumerate(self.trees):\n",
    "            # each tree 't' is already a subset of X conditioned on y=c\n",
    "            # _, n_idx = t.query(X_new, self.k)\n",
    "            n_idx, _ = t.query(X_new)\n",
    "            # Take k nearest neighbors\n",
    "            n_idx = n_idx[:, :self.k]\n",
    "\n",
    "\n",
    "            # Compute centroids\n",
    "            neighbors = self.X[self.c_idx[c][0][n_idx]]\n",
    "            centroid = np.mean(neighbors, axis=-2)\n",
    "            cur_dist = self.dist(X_new, centroid, c)\n",
    "            dist_c[:, c] = cur_dist\n",
    "        \n",
    "        predictions = np.argmin(dist_c, axis=1)\n",
    "        return predictions\n",
    "    \n",
    "    def score(self, X_test, y_test):\n",
    "        y_pred = self.predict(X_test)\n",
    "        return np.average(y_pred == y_test)\n",
    "    \n",
    "    def get_params(self, deep=False):\n",
    "        return {\n",
    "            'k': self.k,\n",
    "            'alpha': self.alpha,\n",
    "            'beta': self.beta,\n",
    "        }\n",
    "    \n",
    "    def set_params(self, **params):\n",
    "        for key, value in params.items():\n",
    "            setattr(self, key, value)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer, MinMaxScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "class DataTransformer:\n",
    "    def __init__(self):\n",
    "        # self.pt = PowerTransformer(method=\"box-cox\")\n",
    "        # self.pt = PowerTransformer(method=\"yeo-johnson\")\n",
    "        self.pt = StandardScaler()\n",
    "        # self.pt = MinMaxScaler()\n",
    "        self.lda = LDA()\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = self.pt.fit_transform(X)\n",
    "        # self.lda.fit(X, y)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = self.pt.transform(X)\n",
    "        # X = self.lda.transform(X)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(338, 3) (338,)\n",
      "k = 3, knn acc = 0.5058823529411764\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import dataset_helpers as ds\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "X, y = ds.get_UCI_dataset(None, dataset_id=763)   # mines\n",
    "# X, y = ds.get_UCI_dataset(None, dataset_id=697)     # student\n",
    "y = np.ravel(y)\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "subset_idx = rng.choice(X.shape[0], min(X.shape[0], 500), replace=False)\n",
    "X = X[subset_idx]\n",
    "y = y[subset_idx]\n",
    "\n",
    "print(X.shape, y.shape, flush=True)\n",
    "\n",
    "knn_acc = []\n",
    "k = 3\n",
    "beta_vals = np.arange(6)\n",
    "sss = StratifiedShuffleSplit(n_splits=10, test_size=0.1, random_state=42)\n",
    "for i, (train_index, test_index) in enumerate(sss.split(X, y)):\n",
    "        tf = DataTransformer().fit(X[train_index], y[train_index])\n",
    "\n",
    "        clf = KNN(k)\n",
    "        clf.fit(tf.transform(X[train_index]), y[train_index])\n",
    "        knn_acc.append(clf.score(tf.transform(X[test_index]), y[test_index]))\n",
    "print(f\"k = {k}, knn acc = {np.mean(knn_acc)}\")\n",
    "for beta in beta_vals:\n",
    "    dknn_acc = []\n",
    "    for i, (train_index, test_index) in enumerate(sss.split(X, y)):\n",
    "        tf = DataTransformer().fit(X[train_index], y[train_index])\n",
    "\n",
    "        clf = DKNN(k, beta=beta)\n",
    "        clf.fit(tf.transform(X[train_index]), y[train_index])\n",
    "        dknn_acc.append(clf.score(tf.transform(X[test_index]), y[test_index]))\n",
    "    print(f\"k = {k}, beta = {beta}, dknn acc = {np.mean(dknn_acc)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DKNN\n",
      "k = 3, acc = 0.9444444444444444\n",
      "k = 4, acc = 0.9703703703703704\n",
      "k = 5, acc = 0.961111111111111\n",
      "k = 6, acc = 0.9629629629629628\n",
      "-----------------\n",
      "KNN\n",
      "k = 3, acc = 0.9574074074074073\n",
      "k = 4, acc = 0.9333333333333333\n",
      "k = 5, acc = 0.9351851851851853\n",
      "k = 6, acc = 0.951851851851852\n",
      "-----------------\n",
      "Centroid\n",
      "acc = 0.9611111111111112\n"
     ]
    }
   ],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=10, test_size=0.3)\n",
    "print(\"DKNN\")\n",
    "for k in range(3, 7):\n",
    "    dknn_acc = []\n",
    "    for i, (train_index, test_index) in enumerate(sss.split(X, y)):\n",
    "        clf = DKNN(k)\n",
    "        tf = DataTransformer().fit(X[train_index], y[train_index])\n",
    "\n",
    "        clf.fit(tf.transform(X[train_index]), y[train_index])\n",
    "        dknn_acc.append(clf.score(tf.transform(X[test_index]), y[test_index]))\n",
    "        # clf.fit(X[train_index], y[train_index])\n",
    "        # acc.append(clf.score(X[test_index], y[test_index]))\n",
    "    print(f\"k = {k}, acc = {np.mean(dknn_acc)}\")\n",
    "print(\"-----------------\")\n",
    "print(\"KNN\")\n",
    "for k in range(3, 7):\n",
    "    dknn_acc = []\n",
    "    for i, (train_index, test_index) in enumerate(sss.split(X, y)):\n",
    "        clf = KNN(k)\n",
    "        tf = DataTransformer().fit(X[train_index], y[train_index])\n",
    "\n",
    "        clf.fit(tf.transform(X[train_index]), y[train_index])\n",
    "        dknn_acc.append(clf.score(tf.transform(X[test_index]), y[test_index]))\n",
    "    print(f\"k = {k}, acc = {np.mean(dknn_acc)}\")\n",
    "\n",
    "print(\"-----------------\")\n",
    "print(\"Centroid\")\n",
    "dknn_acc = []\n",
    "for i, (train_index, test_index) in enumerate(sss.split(X, y)):\n",
    "    clf = NearestCentroid()\n",
    "    tf = DataTransformer().fit(X[train_index], y[train_index])\n",
    "\n",
    "    clf.fit(tf.transform(X[train_index]), y[train_index])\n",
    "    dknn_acc.append(clf.score(tf.transform(X[test_index]), y[test_index]))\n",
    "print(f\"acc = {np.mean(dknn_acc)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for ionosphere:\n",
      "\n",
      "SVM Mean Accuracy (from 10 runs): 0.869 (std: 0.029)\n",
      "LDA Mean Accuracy (from 10 runs): 0.862 (std: 0.029)\n",
      "CART Mean Accuracy (from 10 runs): 0.877 (std: 0.020)\n",
      "KNN Mean Accuracy (from 10 runs): 0.843 (std: 0.020)\n",
      "paper deltas:\n",
      "\tSVM: 0.017868\n",
      "\tLDA: -0.002736\n",
      "\tKNN: -0.014604\n",
      "\n",
      "Results for sonar:\n",
      "\n",
      "SVM Mean Accuracy (from 10 runs): 0.775 (std: 0.048)\n",
      "LDA Mean Accuracy (from 10 runs): 0.737 (std: 0.073)\n",
      "CART Mean Accuracy (from 10 runs): 0.678 (std: 0.033)\n",
      "KNN Mean Accuracy (from 10 runs): 0.798 (std: 0.037)\n",
      "paper deltas:\n",
      "\tSVM: 0.023603\n",
      "\tLDA: -0.000492\n",
      "\tKNN: \u001b[91m-0.066587\u001b[0m\n",
      "\n",
      "Results for balance scale:\n",
      "\n",
      "SVM Mean Accuracy (from 10 runs): 0.915 (std: 0.022)\n",
      "LDA Mean Accuracy (from 10 runs): 0.872 (std: 0.025)\n",
      "CART Mean Accuracy (from 10 runs): 0.778 (std: 0.015)\n",
      "KNN Mean Accuracy (from 10 runs): 0.796 (std: 0.019)\n",
      "paper deltas:\n",
      "\tSVM: 0.015894\n",
      "\tLDA: \u001b[91m0.163340\u001b[0m\n",
      "\tKNN: \u001b[91m-0.078723\u001b[0m\n",
      "\n",
      "Results for wine:\n",
      "\n",
      "SVM Mean Accuracy (from 10 runs): 0.931 (std: 0.019)\n",
      "LDA Mean Accuracy (from 10 runs): 0.981 (std: 0.020)\n",
      "CART Mean Accuracy (from 10 runs): 0.881 (std: 0.054)\n",
      "KNN Mean Accuracy (from 10 runs): 0.685 (std: 0.054)\n",
      "paper deltas:\n",
      "\tSVM: -0.018519\n",
      "\tLDA: 0.005481\n",
      "\tKNN: -0.046815\n",
      "\n",
      "Results for iris:\n",
      "\n",
      "SVM Mean Accuracy (from 10 runs): 0.976 (std: 0.027)\n",
      "LDA Mean Accuracy (from 10 runs): 0.982 (std: 0.013)\n",
      "CART Mean Accuracy (from 10 runs): 0.938 (std: 0.017)\n",
      "KNN Mean Accuracy (from 10 runs): 0.956 (std: 0.020)\n",
      "paper deltas:\n",
      "\tSVM: 0.015556\n",
      "\tLDA: 0.000222\n",
      "\tKNN: -0.013444\n",
      "\n",
      "Results for seeds:\n",
      "\n",
      "SVM Mean Accuracy (from 10 runs): 0.905 (std: 0.030)\n",
      "LDA Mean Accuracy (from 10 runs): 0.967 (std: 0.027)\n",
      "CART Mean Accuracy (from 10 runs): 0.905 (std: 0.039)\n",
      "KNN Mean Accuracy (from 10 runs): 0.854 (std: 0.033)\n",
      "paper deltas:\n",
      "\tSVM: \u001b[91m-0.055238\u001b[0m\n",
      "\tLDA: \u001b[91m0.272667\u001b[0m\n",
      "\tKNN: \u001b[91m-0.054032\u001b[0m\n",
      "\n",
      "Results for knowledge:\n",
      "\n",
      "SVM Mean Accuracy (from 10 runs): 0.835 (std: 0.024)\n",
      "LDA Mean Accuracy (from 10 runs): 0.902 (std: 0.027)\n",
      "CART Mean Accuracy (from 10 runs): 0.875 (std: 0.025)\n",
      "KNN Mean Accuracy (from 10 runs): 0.812 (std: 0.019)\n",
      "paper deltas:\n",
      "\tSVM: \u001b[91m-0.087289\u001b[0m\n",
      "\tLDA: \u001b[91m-0.050521\u001b[0m\n",
      "\tKNN: -0.028430\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import dataset_helpers as ds\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "def print_deltas(results, ds_name):\n",
    "    def red_string(txt):\n",
    "        return f\"\\033[91m{txt:3f}\\033[0m\"\n",
    "    # prints deltas and marks red if delta is far from a single standard deviation\n",
    "    d_svm = results['SVM'][0] - ds.orig_stats[ds_name]['SVM'][0] # [0] is mean [1] is std\n",
    "    d_lda = results['LDA'][0] - ds.orig_stats[ds_name]['LDA'][0]\n",
    "    d_knn = results['KNN'][0] - ds.orig_stats[ds_name]['KNN'][0]\n",
    "\n",
    "    # color red if the deltas are too large\n",
    "    d_svm = f\"{d_svm:3f}\" if abs(d_svm) <= ds.orig_stats[ds_name]['SVM'][1] else red_string(d_svm)\n",
    "    d_lda = f\"{d_lda:3f}\" if abs(d_lda) <= ds.orig_stats[ds_name]['LDA'][1] else red_string(d_lda)\n",
    "    d_knn = f\"{d_knn:3f}\" if abs(d_knn) <= ds.orig_stats[ds_name]['KNN'][1] else red_string(d_knn)\n",
    "\n",
    "    print(\"paper deltas:\\n\"\n",
    "          f\"\\tSVM: {d_svm}\\n\"\n",
    "          f\"\\tLDA: {d_lda}\\n\"\n",
    "          f\"\\tKNN: {d_knn}\\n\"\n",
    "    )\n",
    "\n",
    "# for all datasets we have, get baseline stats\n",
    "for ds_name in ds.name_to_id.keys():\n",
    "    paper_stats = ds.orig_stats[ds_name]\n",
    "    print(f\"Results for {ds_name}:\\n\")\n",
    "    data, labels = ds.get_UCI_dataset(ds_name)\n",
    "    result = ds.run_baseline_models(\n",
    "        data, labels.ravel(),\n",
    "        data_preprocessor=None,\n",
    "        verbose=True)\n",
    "    print_deltas(result, ds_name)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2-dim DKNN(mean: 0.806250, std: 0.395235)\n",
      "\n",
      "SVM Mean Accuracy (from 10 runs): 0.850 (std: 0.357)\n",
      "LDA Mean Accuracy (from 10 runs): 0.838 (std: 0.369)\n",
      "CART Mean Accuracy (from 10 runs): 0.731 (std: 0.443)\n",
      "KNN Mean Accuracy (from 10 runs): 0.794 (std: 0.405)\n",
      "\n",
      "3-dim DKNN(mean: 0.837500, std: 0.368909)\n",
      "\n",
      "SVM Mean Accuracy (from 10 runs): 0.869 (std: 0.338)\n",
      "LDA Mean Accuracy (from 10 runs): 0.875 (std: 0.331)\n",
      "CART Mean Accuracy (from 10 runs): 0.769 (std: 0.422)\n",
      "KNN Mean Accuracy (from 10 runs): 0.831 (std: 0.375)\n",
      "\n",
      "4-dim DKNN(mean: 0.868750, std: 0.337674)\n",
      "\n",
      "SVM Mean Accuracy (from 10 runs): 0.887 (std: 0.316)\n",
      "LDA Mean Accuracy (from 10 runs): 0.887 (std: 0.316)\n",
      "CART Mean Accuracy (from 10 runs): 0.881 (std: 0.323)\n",
      "KNN Mean Accuracy (from 10 runs): 0.831 (std: 0.375)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# NOTE: everything below evaluated with LOOCV rather than the 70/30 split like above\n",
    "# (following the paper)\n",
    "\n",
    "dknn_clf = DKNN(k=6)\n",
    "for dim in range(2, 5): # 2 to 4D synthetic datasets\n",
    "    data, labels = ds.get_synthetic_dataset(dim, seed=None)\n",
    "    loo = LeaveOneOut()\n",
    "    dknn_acc = []\n",
    "    for i, (train_index, test_index) in enumerate(loo.split(data, labels)):\n",
    "        dknn_clf.fit(data[train_index], labels[train_index].astype(int))\n",
    "        dknn_acc.append(dknn_clf.score(data[test_index], labels[test_index]))\n",
    "    print(f\"\\n{dim}-dim DKNN(mean: {np.mean(dknn_acc):3f}, std: {np.std(dknn_acc):3f})\\n\")\n",
    "    # baselines\n",
    "    baseline_results = ds.run_baseline_models(\n",
    "        data, labels.ravel(),\n",
    "        data_preprocessor=None,\n",
    "        verbose=True,\n",
    "        loocv=True)\n",
    "    # print(f\"SVM : {baseline_results['SVM'][0]:3f}\")\n",
    "    # print(f\"LDA : {baseline_results['LDA'][0]:3f}\")\n",
    "    # print(f\"CART: {baseline_results['CART'][0]:3f}\")\n",
    "    # print(f\"KNN : {baseline_results['KNN'][0]:3f}\")\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "random",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
