{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "X, y = load_wine(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial import KDTree\n",
    "import cvxpy as cp\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class DKNN(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, k, alpha=1, beta=1):\n",
    "        super().__init__()\n",
    "        self.k = k          # 'k' neighbors\n",
    "        self.A = None       # PSD matrix objective\n",
    "        self.pi = None      # technically log(pi)\n",
    "        self.trees = []     # search tree for NN\n",
    "        \n",
    "        # importance weights for each class (k,)\n",
    "        if type(alpha) in {float, int}:\n",
    "            self.alpha = np.full(k, alpha)\n",
    "        else:\n",
    "            assert k == len(alpha)\n",
    "            self.alpha = alpha\n",
    "        self.beta  = beta   # regularization term\n",
    "\n",
    "    # Mahalanobis distance\n",
    "    def dist(self, x, mu, c):\n",
    "        delta = x - mu\n",
    "        return np.sum(np.multiply(delta @ self.A, delta), axis=-1) - self.pi[c]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.trees = [] # reset for each fit -- when using CV need this\n",
    "        self.X = X\n",
    "        self.C = np.unique(y)\n",
    "        self.classes_ = self.C\n",
    "        self.c_idx = []  # indices belonging to 'c' w.r.t full training X\n",
    "        for ci in self.C:\n",
    "            self.c_idx.append(np.where(y == ci))\n",
    "        n, d = X.shape\n",
    "\n",
    "        centroids = []\n",
    "\n",
    "        # Find centroids of class C[i]\n",
    "        for idx in self.c_idx:\n",
    "            # Get k nearest neighbors of class C[i] for all training data X\n",
    "            tree = KDTree(X[idx])\n",
    "            _, n_idx = tree.query(X, self.k)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "            # Compute centroids\n",
    "            neighbors = X[idx][n_idx] # X[of class 'c'][its nearest neighbors w.r.t X[c]]\n",
    "            if self.k == 1:\n",
    "                centroid_c = neighbors\n",
    "            else:\n",
    "                centroid_c = np.mean(neighbors, axis=1)\n",
    "            centroids.append(centroid_c)\n",
    "        \n",
    "        centroids = np.stack(centroids, axis=0)\n",
    "\n",
    "        # Convex problem formulation\n",
    "        self.pi = np.array([len(idx[0]) / n for idx in self.c_idx])\n",
    "        self.A = cp.Variable((d, d))\n",
    "\n",
    "        delta = X - centroids\n",
    "\n",
    "        # should work\n",
    "        # f_mult = np.sum(np.multiply(delta @ self.A, delta), axis=2) - self.pi[:, np.newaxis]\n",
    "        # print(f_mult[0, 0])\n",
    "\n",
    "        constraints = []\n",
    "        epsilon = cp.Variable(n)\n",
    "        constraints.append(epsilon >= 0)\n",
    "\n",
    "        for i in range(n):\n",
    "            for c in self.C:\n",
    "                if c == y[i]:\n",
    "                    continue\n",
    "                constraints += [\n",
    "                    delta[y[i], i] @ self.A @ delta[y[i], i].T - cp.log(self.pi[y[i]]) + 1 - epsilon[i] \n",
    "                    <= delta[c, i] @ self.A @ delta[c   , i].T - cp.log(self.pi[c])\n",
    "                ]\n",
    "            constraints += [\n",
    "                epsilon[i] >= 0\n",
    "            ]\n",
    "        \n",
    "        alpha_vec = np.array([self.alpha[y_i] for y_i in y])  # corresponding class importance weight\n",
    "        objective = cp.Minimize(cp.sum(cp.multiply(alpha_vec, epsilon)) + self.beta * cp.norm(self.A))\n",
    "\n",
    "        prob = cp.Problem(objective, constraints)\n",
    "        prob.solve()\n",
    "\n",
    "        self.A = self.A.value\n",
    "\n",
    "    def predict(self, X_new):\n",
    "        if X_new.ndim == 1:\n",
    "            n = 1\n",
    "        else:\n",
    "            n = X_new.shape[0]\n",
    "\n",
    "        dist_c = np.empty((n, len(self.trees)))\n",
    "        for c, t in enumerate(self.trees):\n",
    "            # each tree 't' is already a subset of X conditioned on y=c\n",
    "            _, n_idx = t.query(X_new, self.k)\n",
    "\n",
    "            # Compute centroids\n",
    "            neighbors = self.X[self.c_idx[c][0][n_idx]]\n",
    "            centroid = np.mean(neighbors, axis=-2)\n",
    "            cur_dist = self.dist(X_new, centroid, c)\n",
    "            dist_c[:, c] = cur_dist\n",
    "        \n",
    "        predictions = np.argmin(dist_c, axis=1)\n",
    "        return predictions\n",
    "    \n",
    "    def score(self, X_test, y_test):\n",
    "        y_pred = self.predict(X_test)\n",
    "        return np.average(y_pred == y_test)\n",
    "    \n",
    "    def get_params(self, deep=False):\n",
    "        return {\n",
    "            'k': self.k,\n",
    "            'alpha': self.alpha,\n",
    "            'beta': self.beta,\n",
    "        }\n",
    "    \n",
    "    def set_params(self, **params):\n",
    "        for key, value in params.items():\n",
    "            setattr(self, key, value)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset_helpers as ds\n",
    "# data, labels = ds.get_UCI_dataset(\"wine\")\n",
    "# dknn_clf = DKNN(k=3, alpha=[1.0, 0.5, 1.0], beta=0.01)\n",
    "# ds.accuracy_splits(data, labels, dknn_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DKNN\n",
      "k = 3, acc = 0.9666666666666666\n",
      "k = 4, acc = 0.9555555555555555\n",
      "k = 5, acc = 0.9666666666666666\n",
      "k = 6, acc = 0.9777777777777779\n",
      "-----------------\n",
      "KNN\n",
      "k = 3, acc = 0.9444444444444444\n",
      "k = 4, acc = 0.9222222222222223\n",
      "k = 5, acc = 0.9277777777777778\n",
      "k = 6, acc = 0.961111111111111\n",
      "-----------------\n",
      "Centroid\n",
      "acc = 0.9722222222222221\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, ShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "class DataTransformer:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.pt = PowerTransformer(method=\"box-cox\")\n",
    "        self.lda = LDA()\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = self.pt.fit_transform(X)\n",
    "        # self.lda.fit(X, y)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = self.pt.transform(X)\n",
    "        # X = self.lda.transform(X)\n",
    "\n",
    "        return X\n",
    "\n",
    "rs = ShuffleSplit(n_splits=10, test_size=0.1)\n",
    "print(\"DKNN\")\n",
    "for k in range(3, 7):\n",
    "    acc = []\n",
    "    for i, (train_index, test_index) in enumerate(rs.split(X)):\n",
    "        clf = DKNN(k)\n",
    "        tf = DataTransformer().fit(X[train_index], y[train_index])\n",
    "\n",
    "        clf.fit(tf.transform(X[train_index]), y[train_index])\n",
    "        acc.append(clf.score(tf.transform(X[test_index]), y[test_index]))\n",
    "    print(f\"k = {k}, acc = {np.mean(acc)}\")\n",
    "print(\"-----------------\")\n",
    "print(\"KNN\")\n",
    "for k in range(3, 7):\n",
    "    acc = []\n",
    "    for i, (train_index, test_index) in enumerate(rs.split(X)):\n",
    "        clf = KNN(k)\n",
    "        tf = DataTransformer().fit(X[train_index], y[train_index])\n",
    "\n",
    "        clf.fit(tf.transform(X[train_index]), y[train_index])\n",
    "        acc.append(clf.score(tf.transform(X[test_index]), y[test_index]))\n",
    "    print(f\"k = {k}, acc = {np.mean(acc)}\")\n",
    "\n",
    "print(\"-----------------\")\n",
    "print(\"Centroid\")\n",
    "acc = []\n",
    "for i, (train_index, test_index) in enumerate(rs.split(X)):\n",
    "    clf = NearestCentroid()\n",
    "    tf = DataTransformer().fit(X[train_index], y[train_index])\n",
    "\n",
    "    clf.fit(tf.transform(X[train_index]), y[train_index])\n",
    "    acc.append(clf.score(tf.transform(X[test_index]), y[test_index]))\n",
    "print(f\"acc = {np.mean(acc)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "random",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
